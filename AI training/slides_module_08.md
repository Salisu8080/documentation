# Module 8: Critical Evaluation of AI Outputs
**Estimated Time:** 2.5 hours | **XP Reward:** 650

---

## Slide 1: Module Title

**Critical Evaluation of AI Outputs**

- Understanding AI hallucinations and why they happen
- Fact-checking strategies for academic content
- Evaluating output quality: accuracy, relevance, completeness
- Hands-on: Hallucination Detective

> Speaker notes: This is one of the most important modules. Without these skills, participants risk incorporating false information into their academic work. Take this module seriously.

---

## Slide 2: What Are AI Hallucinations?

**Plausible-sounding but factually incorrect information generated by AI**

**Examples in academic contexts:**
- Fabricated citations (fake author names, titles, journals)
- Misattributed findings (real paper, wrong description)
- Invented statistics ("73% of universities...")
- False historical claims
- Non-existent institutions or standards

> **AI presents hallucinations with the SAME confidence as accurate information.**

> Speaker notes: Start with a real example if you have one. Fabricated citations are the most common and most dangerous academic hallucination.

---

## Slide 3: Why LLMs Hallucinate

**Four fundamental reasons:**

1. **LLMs predict text, not facts.** When asked for a citation, AI predicts what a citation *should look like* — not what actually exists.

2. **No concept of truth.** AI learned from both factual and fictional text. It cannot distinguish between them.

3. **Confidence does not indicate accuracy.** There is no hesitation or signal that says "I made this up."

4. **Specificity increases risk.** Asking for exact page numbers or statistics pushes AI to generate specific content even when it doesn't have accurate information.

> Speaker notes: This is crucial conceptual understanding. Hallucinations are structural — they're how LLMs work, not bugs. Every current AI model hallucinates.

---

## Slide 4: High-Risk Areas for Academics

| Area | Risk Level | Example |
|------|-----------|---------|
| **Citations** | Very High | AI invents author names, paper titles, journals |
| **Statistics** | High | "Studies show 73% of..." with no source |
| **Historical dates** | Medium-High | Wrong dates presented confidently |
| **Quotes** | High | Fabricated quotes attributed to real people |
| **Institutional info** | Medium | Non-existent policies or standards |

> **Literature reviews are the highest-risk academic task for hallucination.**

> Speaker notes: Walk through each area with an example. Most participants will have encountered at least one of these without realizing it was a hallucination.

---

## Slide 5: Real Hallucination Examples

**Example 1 — Fabricated Citation:**
AI generates: "Smith, J. & Johnson, K. (2023). Artificial Intelligence in Higher Education. *Journal of Educational Technology*, 45(2), 112-134."
**Reality:** This paper, these authors, and this journal issue do not exist.

**Example 2 — Misattributed Finding:**
AI states: "According to Venkatesh et al. (2003), UTAUT explains 70% of variance..."
**Reality:** Real paper, but the specific "70%" claim may be inaccurate.

**Example 3 — Invented Statistic:**
AI responds: "A 2024 UNESCO report found 67% of African universities implemented AI policies."
**Reality:** This report and statistic may be entirely fabricated.

> Speaker notes: These feel real. That's the danger. Without verification, any of these could end up in a published paper.

---

## Slide 6: The Golden Rule

## Never trust AI on factual claims without independent verification.

**Especially:**
- Never cite a paper without verifying it exists
- Never use a statistic without tracing it to its source
- Never quote someone without confirming the quote is real
- Never claim a policy exists without checking the institution's website

> Speaker notes: Display this prominently. This is the single most important rule for academic AI use. Repeat it throughout the rest of the training.

---

## Slide 7: Lesson 2 — Fact-Checking Workflow

**Step 1: Identify claims needing verification**
- Citations, statistics, dates, quotes, policies

**Step 2: Verify with authoritative sources**
- Citations → Google Scholar, DOI lookup, library databases
- Statistics → trace to original report
- Facts → established reference sources
- Quotes → search exact text with attributed source

**Step 3: Document discrepancies**

**Step 4: Replace or remove inaccurate content**

> Speaker notes: Make this systematic. Participants need a repeatable process, not ad-hoc checking.

---

## Slide 8: Verification Tools

| What to Verify | Tool to Use |
|---------------|------------|
| Citations exist | Google Scholar, your library database |
| DOI is valid | doi.org |
| Citation metadata | CrossRef (crossref.org) |
| Statistics source | Original organization's website |
| Current information | Perplexity (provides sourced answers) |
| General facts | Multiple independent sources |

> Speaker notes: Demonstrate Google Scholar search for a citation. Show how to spot a fabricated citation (no results found).

---

## Slide 9: Red Flags for Hallucination

**Watch for these warning signs:**

- Overly specific numbers with no source ("73.4% of universities...")
- Perfect-sounding citations you've never encountered in your field
- Evidence that perfectly supports every point (too convenient)
- Inconsistent details (same paper described differently)
- Very recent claims with specific details (2024-2026)

> **If something seems too perfect or too convenient, verify it.**

> Speaker notes: Help participants develop intuition. The "too perfect" test is surprisingly effective — real research rarely has evidence that neatly supports every argument.

---

## Slide 10: Reducing Hallucination Risk

**Safer prompting strategies:**

| Risky Prompt | Safer Alternative |
|-------------|------------------|
| "Give me 10 references with full APA citations" | "What are the major themes in research on [topic]? Don't provide citations — I'll find papers myself" |
| "What does the research say about X?" | "What are the key debates about X? I will verify sources through Google Scholar" |
| "Write a literature review paragraph with citations" | "Suggest how to organize my literature review into themes" |

> **Use AI for ideas and structure. Handle citations yourself.**

> Speaker notes: This is a paradigm shift for many participants. Don't ask AI for citations — ask for concepts and find the citations yourself.

---

## Slide 11: Lesson 3 — Quality Evaluation Framework

**Evaluate every AI output on three dimensions:**

| Dimension | Key Question |
|-----------|-------------|
| **Accuracy** | Are the facts correct? Is reasoning sound? |
| **Relevance** | Does it address MY specific question and context? |
| **Completeness** | Are there gaps? Missing perspectives? |

**Score each 1-5. Below 3 on any dimension = refine or abandon.**

> Speaker notes: This framework applies to all AI use, not just hallucination checking. It's a general quality assurance tool.

---

## Slide 12: When to Refine vs. Abandon

**Refine your prompt when:**
- Output is relevant but lacks depth → add "provide more detail"
- Format is wrong → add format specifications
- Level is off → specify audience more clearly
- Missing aspects → list required components

**Abandon the AI approach when:**
- Consistently inaccurate on this topic
- Task requires real-time information AI doesn't have
- Field-specific nuance AI cannot provide
- 3+ prompt refinements without improvement

> Speaker notes: Knowing when to stop using AI for a task is as important as knowing how to use it. Not every task benefits from AI.

---

## Slide 13: Activity — Hallucination Detective

**Objective:** Test AI for hallucinations in YOUR field and practice verification

**What you'll do:**
1. Choose a topic you know well
2. Ask AI for a literature review paragraph with 4+ citations
3. Verify EVERY citation on Google Scholar
4. Check factual claims against independent sources
5. Rate: Accuracy, Relevance, Completeness (1-5 each)
6. Write a verification report

**Time:** 50 minutes

> Speaker notes: This is an eye-opening activity. Many participants will discover fabricated citations for the first time. Ensure everyone has internet access for Google Scholar.

---

## Slide 14: Activity — Verification Template

| Citation # | Text from AI | Exists? (Google Scholar) | Claims Match? | Notes |
|-----------|-------------|-------------------------|---------------|-------|
| 1 | | Yes / No | Yes / No / N/A | |
| 2 | | Yes / No | Yes / No / N/A | |
| 3 | | Yes / No | Yes / No / N/A | |
| 4 | | Yes / No | Yes / No / N/A | |

**Factual claims check:**
| Claim | Verified? | Source | Accurate? |
|-------|----------|--------|-----------|
| | | | |

> Speaker notes: Provide this as a handout. Structured verification is more thorough than informal checking.

---

## Slide 15: Activity Debrief

**Discussion:**

1. How many AI citations were real? Were you surprised?
2. Did you find any claims you initially believed but were wrong?
3. How will this change how you use AI for research?
4. What personal fact-checking routine will you adopt?

**Typical findings:** 30-70% of AI-generated citations are fabricated or inaccurate. This usually shocks participants.

> Speaker notes: This is a powerful learning moment. Let participants share their findings. The shock of discovering fabricated citations creates lasting awareness.

---

## Slide 16: Module 8 Summary

**Key Takeaways:**

- AI hallucinations are structural — all LLMs produce them
- Citations, statistics, and quotes are the highest-risk areas
- **Never cite a source based on AI's description alone**
- Follow a systematic fact-checking workflow for all AI-generated academic content
- Use the Accuracy-Relevance-Completeness framework to evaluate all AI outputs
- Use AI for ideas and structure; handle citations and verification yourself

**Next Module:** Ethics, Academic Integrity and Policy
