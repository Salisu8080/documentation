# Module 9: Ethics, Academic Integrity and Policy
**Estimated Time:** 2.5 hours | **XP Reward:** 650

---

## Slide 1: Module Title

**Ethics, Academic Integrity and Policy**

- Academic integrity in the age of AI
- Ethical use of AI in teaching and research
- Data privacy and security
- Hands-on: Draft an AI use policy

> Speaker notes: This module addresses the concerns participants raised most in the pre-training survey — integrity, plagiarism, and ethical guidelines. It's both practical and philosophical.

---

## Slide 2: The Academic Integrity Challenge

**AI has complicated how we define academic integrity:**

- AI can generate text indistinguishable from human writing
- The line between "using a tool" and "having the tool do the work" is blurry
- Plagiarism detection tools struggle with AI-generated text
- Policies haven't kept pace with technology
- Different institutions have vastly different standards

> **The fundamental question:** When does using AI as a tool become using AI as a substitute for learning or original work?

> Speaker notes: Open with a brief discussion. Ask: "Have any of you encountered this dilemma already?" Most will have stories.

---

## Slide 3: The Spectrum of AI Use

| Generally Acceptable | Ethically Gray | Generally Prohibited |
|---------------------|---------------|---------------------|
| Brainstorming and ideation | Substantial drafting assistance | Submitting AI text as your own |
| Outlining and structuring | AI-generated assessments without adaptation | Using AI to complete student assignments |
| Grammar and style editing | Heavy reliance on AI for research direction | Fabricating data with AI |
| Generating examples | AI-generated data analysis | Undisclosed AI use in publications |
| Summarizing your own work | Paraphrasing to avoid detection | Bypassing peer review with AI |
| Translation assistance | | |

> Speaker notes: Walk through each column. The gray area generates the most discussion. Encourage participants to share where they draw the line.

---

## Slide 4: The Transparency Principle

**The most widely accepted ethical standard:**

## Disclose how you used AI and to what extent.

**In practice:**
- **Publications:** AI Use Statement (many journals now require this)
- **Teaching:** Course syllabi should state your AI policy
- **Grant applications:** Disclose AI assistance as required
- **Student work:** Students acknowledge AI like any other assistance

> **When in doubt, disclose.**

> Speaker notes: Transparency is the safest approach. It's easier to over-disclose than to be accused of concealing AI use.

---

## Slide 5: Case Studies

**Case 1 — Acceptable:**
Dr. Amina uses ChatGPT to brainstorm discussion questions. She evaluates, modifies, and adds her own. She tells students AI helped with initial ideas.
*Why acceptable:* AI was a brainstorming tool. Final product reflects her judgment. Transparent.

**Case 2 — Gray Area:**
Prof. Bello asks AI to draft his literature review, then extensively edits, verifies citations, and rewrites most sentences. He discloses.
*Why gray:* Structure originated from AI. Increasingly accepted with disclosure, but debatable.

**Case 3 — Unethical:**
A PhD student has AI generate an entire thesis chapter, makes minimal changes, submits without disclosure.
*Why unethical:* Does not represent learning or original contribution. No transparency.

> Speaker notes: Use these cases for group discussion. Ask: "Where would YOU draw the line? What makes Case 2 different from Case 3?"

---

## Slide 6: What Universities Are Doing

| Approach | Description | Example |
|----------|------------|---------|
| **Restrictive** | Ban AI in student submissions | Some US/UK universities (early 2023) |
| **Permissive** | Allow AI with mandatory disclosure | Many Australian universities |
| **Task-specific** | Different rules per assignment type | Growing trend globally |
| **No policy yet** | No formal guidance | Many institutions, especially in developing countries |

> **The trend:** Moving toward permissive-with-disclosure, recognizing AI use is inevitable.

> Speaker notes: Ask participants about their own institutions. Many Nigerian universities don't yet have policies — this module helps them develop one.

---

## Slide 7: Lesson 2 — Ethical Use in Teaching

**Key ethical concerns:**

**Equity and Access:**
- Not all students can afford premium AI tools
- Internet access varies
- Students with paid AI have an advantage

**Learning Rigor:**
- AI shortcuts may prevent deep understanding
- Writing skills may atrophy without practice
- Critical thinking may be bypassed

**Assessment Integrity:**
- Take-home essays can be AI-generated
- Traditional exams may need redesigning

> Speaker notes: These are real challenges, not hypothetical. Discuss practical solutions for each concern.

---

## Slide 8: Ethical Solutions for Teaching

| Problem | Solution |
|---------|---------|
| Unequal AI access | Provide institutional access or identify free alternatives |
| Learning bypass | Require process documentation, not just final products |
| Assessment gaming | Use oral exams, in-class work, reflective journals |
| AI-generated submissions | Design assignments requiring personal experience/reflection |

> **Design assignments that remain meaningful even with AI access.**

> Speaker notes: The most effective approach is designing AI-resistant assessments rather than trying to ban AI use.

---

## Slide 9: Ethical Use in Research

**Key questions:**

- **Authorship:** Can AI be listed as co-author? (Most journals say NO — AI cannot take responsibility)
- **Acknowledgment:** How to acknowledge AI contributions?
- **Bias:** AI can perpetuate biases from training data
- **Context:** Western-centric training data may not suit African research contexts

> **Emerging consensus:** Acknowledge AI in methods or acknowledgments section. Do not list as author.

> Speaker notes: The authorship question is settled for now — AI is not an author. The acknowledgment question is evolving rapidly.

---

## Slide 10: Bias in AI Outputs

**AI tools can perpetuate and amplify biases:**

- Training data reflects historical inequities
- Content may stereotype or marginalize groups
- Research suggestions may favor well-represented fields
- Western-centric defaults may not suit your context

**Your responsibility:**
> "Whose perspective is missing here?"
> "Does this reflect MY context or a default Western perspective?"

> Speaker notes: This is especially relevant for African academics. AI trained primarily on Western data may not produce culturally appropriate content. Always critically evaluate.

---

## Slide 11: Lesson 3 — Data Privacy and Security

**When you type into AI tools, your data may be:**

- Stored on company servers
- Used to train future models (some tools, some settings)
- Accessible to company employees
- Subject to data breach risk

> **Key question:** Would it be acceptable if this data became public?

> Speaker notes: Most participants haven't thought about this. Creating awareness is the primary goal.

---

## Slide 12: Data You Should NEVER Input

| Category | Examples |
|----------|---------|
| **Student data** | Names, matric numbers, grades, personal info |
| **Confidential research** | Unpublished findings, raw data with identifiers, interview transcripts |
| **Institutional info** | Personnel records, budgets, exam papers before administration |
| **Peer review content** | Manuscripts under review, reviewer comments |

> **Rule of thumb:** Assume anything you type could become public.

> Speaker notes: Walk through each category with specific examples. Many participants regularly paste student data or exam questions into AI tools without considering the risk.

---

## Slide 13: Safe Practices

1. **Anonymize** all data before input (replace names with codes)
2. **Use institutional AI tools** when available (stronger data agreements)
3. **Opt out** of training data sharing when possible
4. **Don't paste entire documents** — extract and rephrase relevant parts
5. **Use fictional examples** when developing prompts for sensitive topics

**Example of safe approach:**
> Instead of: pasting actual student evaluations with names
> Do: "Student 1 commented that the pace was too fast. Student 2 appreciated real-world examples..."

> Speaker notes: Practical, actionable advice. The anonymization example shows exactly how to handle real scenarios.

---

## Slide 14: AI Tool Privacy Comparison

| Tool | Training Data Policy | Notes |
|------|---------------------|-------|
| ChatGPT (Free) | May use conversations for training | Can opt out in settings |
| ChatGPT (Enterprise) | Does NOT use data for training | Institutional license needed |
| Claude | Does NOT use conversations by default | Check current policy |
| Gemini | Depends on account type | Personal vs. Workspace differs |
| Microsoft Copilot | Enterprise versions have stronger protections | Personal versions less protected |

> **Always check current policies — they change frequently.**

> Speaker notes: Policies evolve. The point is awareness, not memorizing specific policies. Always check before using a tool with sensitive data.

---

## Slide 15: Activity — Draft an AI Use Policy

**Objective:** Create a practical AI use policy for your department or course

**Your policy must include:**
1. Purpose and scope
2. Acceptable uses (5-6 specific examples)
3. Prohibited uses (4-5 specific examples)
4. Disclosure requirements
5. Consequences for violations
6. Support and resources

**Then:** Critically review, revise, and plan how to communicate it

**Time:** 50 minutes

> Speaker notes: This is the most impactful activity for institutional change. Participants leave with a draft policy they can actually implement.

---

## Slide 16: Activity — Policy Prompt

> You are a higher education policy specialist. Help me draft a responsible AI use policy for [MY CONTEXT].
>
> Include: purpose and scope, acceptable uses (5-6 examples), prohibited uses (4-5 examples), disclosure requirements, consequences, support resources.
>
> Context: [INSTITUTION TYPE] in Nigeria. Policy should be practical, enforceable, and balanced. Consider varying student access to AI tools.

**After AI generates:** Review for your specific context, add local examples, ensure enforceability.

> Speaker notes: The AI draft is a starting point. Participants must customize for their actual department, students, and institutional culture.

---

## Slide 17: Activity Debrief

**Discussion:**

1. How would you communicate this policy to students or colleagues?
2. What challenges do you foresee in enforcement?
3. How often should the policy be reviewed?
4. What was the AI draft missing about YOUR specific context?

> Speaker notes: Share examples of well-crafted policies. Discuss the balance between restrictive and permissive approaches.

---

## Slide 18: Module 9 Summary

**Key Takeaways:**

- AI use falls on a spectrum — not all use is cheating, not all use is fine
- **Transparency is the safest principle** — when in doubt, disclose
- Address equity: ensure all students have AI access if you allow AI use
- Evaluate AI outputs for bias, especially cultural and contextual bias
- **Never input sensitive data** into public AI tools
- Draft and implement an AI policy now — don't wait for institutional guidance

**Next Module:** Building Your AI-Enhanced Academic Workflow
